{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc31b6f9",
   "metadata": {},
   "source": [
    "# Generative AI (CS F437) Assignment 1\n",
    "Training and Evaluating Transformer Models for English to Hindi Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624478a",
   "metadata": {},
   "source": [
    "## Part 1: Fine-Tuning a Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598cf6d",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6e8334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m\u001b[m\n",
      "\n",
      "\u001b[H\u001b[2J\n",
      "\u001b[31m\u001b[m\n",
      "\n",
      "\u001b[H\u001b[2JRequirement already satisfied: numpy in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (3.9.4)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.8-py3-none-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (4.48.1)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: click in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nltk) (4.65.2)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting eval-type-backport (from wandb)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (4.23.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (6.1.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: pyyaml in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.25.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (70.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from datasets) (3.16.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests<3,>=2.0.0 (from wandb)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.33.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions<5,>=4.4 (from wandb)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zoddtheimmortal/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading wandb-0.19.8-py3-none-macosx_11_0_arm64.whl (19.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading pyarrow-19.0.1-cp39-cp39-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sentry_sdk-2.25.0-py2.py3-none-any.whl (338 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading setproctitle-1.3.5-cp39-cp39-macosx_11_0_arm64.whl (11 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: xxhash, typing-extensions, tqdm, smmap, setproctitle, sentry-sdk, requests, pyarrow, eval-type-backport, docker-pycreds, dill, annotated-types, typing-inspection, pydantic-core, nltk, multiprocess, gitdb, pydantic, gitpython, wandb, datasets, evaluate\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.2\n",
      "    Uninstalling tqdm-4.65.2:\n",
      "      Successfully uninstalled tqdm-4.65.2\n",
      "  Attempting uninstall: sentry-sdk\n",
      "    Found existing installation: sentry-sdk 1.14.0\n",
      "    Uninstalling sentry-sdk-1.14.0:\n",
      "      Successfully uninstalled sentry-sdk-1.14.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.9\n",
      "    Uninstalling pydantic-1.10.9:\n",
      "      Successfully uninstalled pydantic-1.10.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "animdl 1.7.27 requires httpx<0.24.0,>=0.23.0, but you have httpx 0.27.2 which is incompatible.\n",
      "animdl 1.7.27 requires packaging<24,>=22, but you have packaging 20.9 which is incompatible.\n",
      "animdl 1.7.27 requires tqdm<4.66.0,>=4.62.3, but you have tqdm 4.67.1 which is incompatible.\n",
      "ipython 8.18.1 requires prompt-toolkit<3.1.0,>=3.0.41, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
      "rasa 3.6.21 requires matplotlib<3.6,>=3.1, but you have matplotlib 3.9.4 which is incompatible.\n",
      "rasa 3.6.21 requires networkx<2.7,>=2.4, but you have networkx 3.2.1 which is incompatible.\n",
      "rasa 3.6.21 requires pydantic<1.10.10, but you have pydantic 2.11.1 which is incompatible.\n",
      "rasa 3.6.21 requires scipy<1.11.0,>=1.10.0; python_version >= \"3.8\" and python_version < \"3.11\", but you have scipy 1.9.1 which is incompatible.\n",
      "rasa 3.6.21 requires sentry-sdk<1.15.0,>=0.17.0, but you have sentry-sdk 2.25.0 which is incompatible.\n",
      "tensorflow-macos 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 datasets-3.5.0 dill-0.3.8 docker-pycreds-0.4.0 eval-type-backport-0.2.2 evaluate-0.4.3 gitdb-4.0.12 gitpython-3.1.44 multiprocess-0.70.16 nltk-3.9.1 pyarrow-19.0.1 pydantic-2.11.1 pydantic-core-2.33.0 requests-2.32.3 sentry-sdk-2.25.0 setproctitle-1.3.5 smmap-5.0.2 tqdm-4.67.1 typing-extensions-4.13.0 typing-inspection-0.4.0 wandb-0.19.8 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install numpy pandas matplotlib nltk wandb datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Download NLTK data required for BLEU computation\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927180c",
   "metadata": {},
   "source": [
    "We use WandB to store out weights and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ace57",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api_key = input(\"Enter your wandb API key: \") \n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# Initialize wandb run (change project and entity as desired)\n",
    "wandb.init(\n",
    "    project=\"GEN_AI\",\n",
    "    entity=\"aashreyrachaputi-bits-pilani\", \n",
    "    config={\n",
    "        \"model_name\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
    "        \"dataset\": \"cfilt/iitb-english-hindi\",\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"num_decoder_layers_to_keep_trainable\": 1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8f4ad",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "Load the IITB English-Hindi parallel corpus and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "train_data = ds[\"train\"]\n",
    "val_data = ds[\"validation\"]\n",
    "test_data = ds[\"test\"]\n",
    "\n",
    "# Create text dictionaries for evaluation\n",
    "train_texts = {\n",
    "    \"en\": [example[\"en\"] for example in train_data[\"translation\"]],\n",
    "    \"hi\": [example[\"hi\"] for example in train_data[\"translation\"]],\n",
    "}\n",
    "val_texts = {\n",
    "    \"en\": [example[\"en\"] for example in val_data[\"translation\"]],\n",
    "    \"hi\": [example[\"hi\"] for example in val_data[\"translation\"]],\n",
    "}\n",
    "test_texts = {\n",
    "    \"en\": [example[\"en\"] for example in test_data[\"translation\"]],\n",
    "    \"hi\": [example[\"hi\"] for example in test_data[\"translation\"]],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa080b",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "Set up the training arguments and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0526c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True, \n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2340b2",
   "metadata": {},
   "source": [
    "### Model Layer Freezing\n",
    "Freeze specific layers in the model to focus training on the most important components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, freeze_encoder=True, num_decoder_layers_to_keep_trainable=2):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if not freeze_encoder:\n",
    "        for param in model.model.encoder.embed_tokens.parameters():\n",
    "            param.requires_grad = True\n",
    "        for layer in model.model.encoder.layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Unfreeze decoder embeddings\n",
    "    for param in model.model.decoder.embed_tokens.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the last N decoder layers only\n",
    "    total_decoder_layers = len(model.model.decoder.layers)\n",
    "    for i in range(total_decoder_layers - num_decoder_layers_to_keep_trainable, total_decoder_layers):\n",
    "        if i >= 0:\n",
    "            for param in model.model.decoder.layers[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Always unfreeze the final output projection\n",
    "    for param in model.lm_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    wandb.log({\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_percentage\": trainable_params/total_params\n",
    "    })\n",
    "    \n",
    "    architecture_info = {\n",
    "        \"encoder_status\": \"Frozen\" if freeze_encoder else \"Trainable\",\n",
    "        \"decoder_status\": f\"Partially trainable (last {num_decoder_layers_to_keep_trainable} layers)\",\n",
    "        \"output_projection\": \"Trainable\"\n",
    "    }\n",
    "    wandb.log({\"model_architecture\": architecture_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layers(model, freeze_encoder=True, num_decoder_layers_to_keep_trainable=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./pretrain_model_7l\")\n",
    "tokenizer.save_pretrained(\"./pretrain_model_7l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe265079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    batch_size = 8\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        # Ensure tensors are on the correct device\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        # Generate translations using beam search\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        translations.extend(decoded_outputs)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c03f2",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Once we have trained the model and saved it to our hardrive, we can directly import it from there to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98985c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "model_dir = \"./pretrain_model_7l\"\n",
    "\n",
    "# Load tokenizer from the folder\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load the model from the folder\n",
    "model = MarianMTModel.from_pretrained(model_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c9bfb",
   "metadata": {},
   "source": [
    "#### Interactive Translation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple translation loop\n",
    "while True:\n",
    "    english_text = input(\"Enter an English sentence (or 'exit' to quit): \")\n",
    "    if english_text.strip().lower() == 'exit':\n",
    "        break\n",
    "    inputs = tokenizer(english_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(**inputs)\n",
    "    hindi_translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    print(\"Hindi Translation:\", hindi_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe40b0",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Evaluate the translation model using standard metrics including:\n",
    "- BLEU score: Measures n-gram overlap between translations and references\n",
    "- ROUGE score: Measures recall of n-grams between translations and references\n",
    "- METEOR score: Measures word-to-word matches between translations and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import evaluate\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\n",
    "source_texts = [item[\"translation\"][\"en\"] for item in dataset]\n",
    "reference_texts = [item[\"translation\"][\"hi\"] for item in dataset]\n",
    "\n",
    "print(len(source_texts))\n",
    "print(len(reference_texts))\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def translate(texts, batch_size=8):\n",
    "    predictions = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=5,\n",
    "                max_length=128,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions.extend(decoded)\n",
    "    return predictions\n",
    "\n",
    "predictions = translate(source_texts)\n",
    "\n",
    "references_tokenized = [[ref.split()] for ref in reference_texts]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "\n",
    "# Compute BLEU score\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "bleu_score = corpus_bleu(references_tokenized, predictions_tokenized, smoothing_function=smoothing_function)\n",
    "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_results = rouge.compute(predictions=predictions, references=reference_texts)\n",
    "print(f\"ROUGE-1 F1 Score: {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L F1 Score: {rouge_results['rougeL']:.4f}\")\n",
    "\n",
    "# Compute METEOR score\n",
    "meteor_score = meteor.compute(predictions=predictions, references=reference_texts)\n",
    "print(f\"METEOR Score: {meteor_score['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7abeb",
   "metadata": {},
   "source": [
    "#### Visualize Results\n",
    "\n",
    "The cell below can be used to visualize translation examples and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "num_examples = 20\n",
    "random_indices = random.sample(range(len(source_texts)), num_examples)\n",
    "\n",
    "examples = []\n",
    "for idx in random_indices:\n",
    "    examples.append({\n",
    "        \"English\": source_texts[idx],\n",
    "        \"Hindi (Reference)\": reference_texts[idx],\n",
    "        \"Hindi (Predicted)\": predictions[idx]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249914",
   "metadata": {},
   "source": [
    "## Part 2: Trained Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a0107",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece sacrebleu sacremoses nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189402a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b2a02",
   "metadata": {},
   "source": [
    "### Loading Dataset and Model\n",
    "\n",
    "We'll use the IITB English-Hindi parallel corpus and the mBART-50 multilingual translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "\n",
    "\n",
    "ds = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "test_data = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained mBART model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set source and target languages\n",
    "tokenizer.src_lang = \"en_XX\"  \n",
    "tokenizer.tgt_lang = \"hi_IN\"\n",
    "print(\"Model and tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf20ad0",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "We'll evaluate the model using three metrics:\n",
    "\n",
    "1. **BLEU** (Bilingual Evaluation Understudy): Measures precision of n-grams\n",
    "2. **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation): Recall-oriented metric for summarization\n",
    "3. **METEOR** (Metric for Evaluation of Translation with Explicit ORdering): Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "def evaluate_model(model, dataset, max_samples=None):\n",
    "    model.eval()\n",
    "    metric_bleu = load(\"sacrebleu\")\n",
    "    metric_rouge = load(\"rouge\")\n",
    "    metric_meteor = load(\"meteor\")\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    eval_dataset = dataset[:max_samples] if max_samples else dataset\n",
    "    total = len(eval_dataset)\n",
    "    \n",
    "    for i, example in enumerate(eval_dataset):\n",
    "        print(f\"Processing... {i+1}/{total} ({(i+1)/total*100:.1f}%)\")\n",
    "        \n",
    "        input_text = example[\"translation\"][\"en\"]\n",
    "        ref_text = example[\"translation\"][\"hi\"]\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs, \n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "                max_length=128\n",
    "            )\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        references.append([ref_text])\n",
    "        predictions.append(pred_text)\n",
    "    \n",
    "    bleu_score = metric_bleu.compute(predictions=predictions, references=references)\n",
    "    rouge_score = metric_rouge.compute(predictions=predictions, references=references)\n",
    "    meteor_score = metric_meteor.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return bleu_score, rouge_score, meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb69d3b",
   "metadata": {},
   "source": [
    "### Running the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee38d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 1000 \n",
    "\n",
    "bleu, rouge, meteor = evaluate_model(model, test_data, MAX_SAMPLES)\n",
    "\n",
    "print(\"\\n===== Evaluation Results =====\\n\")\n",
    "print(f\"BLEU Score: {bleu['score']:.2f}\")\n",
    "print(f\"ROUGE-1 Precision: {rouge['rouge1'].mid.precision:.4f}, Recall: {rouge['rouge1'].mid.recall:.4f}, F1: {rouge['rouge1'].mid.fmeasure:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {rouge['rouge2'].mid.fmeasure:.4f}\")\n",
    "print(f\"ROUGE-L F1: {rouge['rougeL'].mid.fmeasure:.4f}\")\n",
    "print(f\"METEOR Score: {meteor['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7c1bf",
   "metadata": {},
   "source": [
    "### Sample Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c799820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_translations(model, dataset, num_samples=5):\n",
    "    model.eval()\n",
    "    import random\n",
    "    samples = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for i, idx in enumerate(samples):\n",
    "        example = dataset[idx]\n",
    "        input_text = example[\"translation\"][\"en\"]\n",
    "        ref_text = example[\"translation\"][\"hi\"]\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"])\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"English: {input_text}\")\n",
    "        print(f\"Reference Hindi: {ref_text}\")\n",
    "        print(f\"Predicted Hindi: {pred_text}\")\n",
    "\n",
    "show_sample_translations(model, test_data, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
