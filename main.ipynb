{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc31b6f9",
   "metadata": {},
   "source": [
    "# Generative AI (CS F437) Assignment 1\n",
    "Training and Evaluating Transformer Models for English to Hindi Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624478a",
   "metadata": {},
   "source": [
    "## Part 1: Fine-Tuning a Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598cf6d",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pip install --quiet --upgrade pip\n",
    "@pip install numpy pandas matplotlib nltk wandb datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Download NLTK data required for BLEU computation\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927180c",
   "metadata": {},
   "source": [
    "We use WandB to store out weights and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ace57",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api_key = input(\"Enter your wandb API key: \") \n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# Initialize wandb run (change project and entity as desired)\n",
    "wandb.init(\n",
    "    project=\"GEN_AI\",\n",
    "    entity=\"aashreyrachaputi-bits-pilani\", \n",
    "    config={\n",
    "        \"model_name\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
    "        \"dataset\": \"cfilt/iitb-english-hindi\",\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"num_decoder_layers_to_keep_trainable\": 1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8f4ad",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "Load the IITB English-Hindi parallel corpus and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "train_data = ds[\"train\"]\n",
    "val_data = ds[\"validation\"]\n",
    "test_data = ds[\"test\"]\n",
    "\n",
    "# Create text dictionaries for evaluation\n",
    "train_texts = {\n",
    "    \"en\": [example[\"en\"] for example in train_data[\"translation\"]],\n",
    "    \"hi\": [example[\"hi\"] for example in train_data[\"translation\"]],\n",
    "}\n",
    "val_texts = {\n",
    "    \"en\": [example[\"en\"] for example in val_data[\"translation\"]],\n",
    "    \"hi\": [example[\"hi\"] for example in val_data[\"translation\"]],\n",
    "}\n",
    "test_texts = {\n",
    "    \"en\": [example[\"en\"] for example in test_data[\"translation\"]],\n",
    "    \"hi\": [example[\"hi\"] for example in test_data[\"translation\"]],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa080b",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "Set up the training arguments and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0526c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True, \n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2340b2",
   "metadata": {},
   "source": [
    "### Model Layer Freezing\n",
    "Freeze specific layers in the model to focus training on the most important components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, freeze_encoder=True, num_decoder_layers_to_keep_trainable=2):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if not freeze_encoder:\n",
    "        for param in model.model.encoder.embed_tokens.parameters():\n",
    "            param.requires_grad = True\n",
    "        for layer in model.model.encoder.layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Unfreeze decoder embeddings\n",
    "    for param in model.model.decoder.embed_tokens.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the last N decoder layers only\n",
    "    total_decoder_layers = len(model.model.decoder.layers)\n",
    "    for i in range(total_decoder_layers - num_decoder_layers_to_keep_trainable, total_decoder_layers):\n",
    "        if i >= 0:\n",
    "            for param in model.model.decoder.layers[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Always unfreeze the final output projection\n",
    "    for param in model.lm_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    wandb.log({\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_percentage\": trainable_params/total_params\n",
    "    })\n",
    "    \n",
    "    architecture_info = {\n",
    "        \"encoder_status\": \"Frozen\" if freeze_encoder else \"Trainable\",\n",
    "        \"decoder_status\": f\"Partially trainable (last {num_decoder_layers_to_keep_trainable} layers)\",\n",
    "        \"output_projection\": \"Trainable\"\n",
    "    }\n",
    "    wandb.log({\"model_architecture\": architecture_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layers(model, freeze_encoder=True, num_decoder_layers_to_keep_trainable=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./pretrain_model_7l\")\n",
    "tokenizer.save_pretrained(\"./pretrain_model_7l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe265079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    batch_size = 8\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        # Ensure tensors are on the correct device\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        # Generate translations using beam search\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        translations.extend(decoded_outputs)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c03f2",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Once we have trained the model and saved it to our hardrive, we can directly import it from there to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98985c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "model_dir = \"./pretrain_model_7l\"\n",
    "\n",
    "# Load tokenizer from the folder\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load the model from the folder\n",
    "model = MarianMTModel.from_pretrained(model_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c9bfb",
   "metadata": {},
   "source": [
    "#### Interactive Translation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple translation loop\n",
    "while True:\n",
    "    english_text = input(\"Enter an English sentence (or 'exit' to quit): \")\n",
    "    if english_text.strip().lower() == 'exit':\n",
    "        break\n",
    "    inputs = tokenizer(english_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(**inputs)\n",
    "    hindi_translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    print(\"Hindi Translation:\", hindi_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe40b0",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Evaluate the translation model using standard metrics including:\n",
    "- BLEU score: Measures n-gram overlap between translations and references\n",
    "- ROUGE score: Measures recall of n-grams between translations and references\n",
    "- METEOR score: Measures word-to-word matches between translations and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import evaluate\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\n",
    "source_texts = [item[\"translation\"][\"en\"] for item in dataset]\n",
    "reference_texts = [item[\"translation\"][\"hi\"] for item in dataset]\n",
    "\n",
    "print(len(source_texts))\n",
    "print(len(reference_texts))\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def translate(texts, batch_size=8):\n",
    "    predictions = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=5,\n",
    "                max_length=128,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions.extend(decoded)\n",
    "    return predictions\n",
    "\n",
    "predictions = translate(source_texts)\n",
    "\n",
    "references_tokenized = [[ref.split()] for ref in reference_texts]\n",
    "predictions_tokenized = [pred.split() for pred in predictions]\n",
    "\n",
    "# Compute BLEU score\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "bleu_score = corpus_bleu(references_tokenized, predictions_tokenized, smoothing_function=smoothing_function)\n",
    "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_results = rouge.compute(predictions=predictions, references=reference_texts)\n",
    "print(f\"ROUGE-1 F1 Score: {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L F1 Score: {rouge_results['rougeL']:.4f}\")\n",
    "\n",
    "# Compute METEOR score\n",
    "meteor_score = meteor.compute(predictions=predictions, references=reference_texts)\n",
    "print(f\"METEOR Score: {meteor_score['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7abeb",
   "metadata": {},
   "source": [
    "#### Visualize Results\n",
    "\n",
    "The cell below can be used to visualize translation examples and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "num_examples = 20\n",
    "random_indices = random.sample(range(len(source_texts)), num_examples)\n",
    "\n",
    "examples = []\n",
    "for idx in random_indices:\n",
    "    examples.append({\n",
    "        \"English\": source_texts[idx],\n",
    "        \"Hindi (Reference)\": reference_texts[idx],\n",
    "        \"Hindi (Predicted)\": predictions[idx]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(examples)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
